{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b241106-d847-454b-8421-5b9c2b753411",
   "metadata": {},
   "source": [
    "- reference : \n",
    "    - [Youtube : Julia Tutorial 2022 Korea 줄리아 언어 소개](https://www.youtube.com/watch?v=wjLXmz4D9VQ&t=2131s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5061054a-84ff-4bc7-8380-4eb9bbbc811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed computing in Julia\n",
    "- 김기백 박사 (Argonne National Laboratory)\n",
    "---\n",
    "\n",
    "- 줄리아 `Distributed` 패키지에서 스케줄링도 가능\n",
    "- `MPI` 패키지 + shell 스크립트를 이용하면 `C`만큼의 퍼포먼스가 나옴.\n",
    "- `C`에 비해 Julia의 편의성이 월등하고, 퍼포먼스는 주의 깊은 코딩으로 달성 가능하다.\n",
    "- multiple dispatch를 통해 자료형을 쉽게 변환하고 최적화된 연산을 수행할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b5bdc-6b23-49b1-9cf8-a8dcb133f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Julia + Deep Learning\n",
    "- Flux.jl \n",
    "    - 줄리아 100% 연산 라이브러리\n",
    "    - 가볍고 기본에 충실한 딥러닝 프레임워크(Jax와 유사)\n",
    "    - 복잡한 모델이나 다른 기능을 원한다면 Flux.jl 기반 다른 패키지를 사용하거나 직접 작성\n",
    "        - Transformer : Transformers.jl\n",
    "        - Geometric Deep learning : Flux.jl\n",
    "    - Flux.jl을 쓰면 좋은 경우\n",
    "        - 시뮬레이션과 결합한 딥러닝\n",
    "        - 내부 Deep Learning 코드를 건드려야하는 경우\n",
    "          ex - LSTM 수식 자체를 수정해야하는 경우\n",
    "        - Automatic differentiation에 관련된 프로젝트\n",
    "        - 파이썬보다 parallel computing이 더 간단\n",
    "    - Flux.jl을 쓰면 안 좋은 경우\n",
    "        - 일반적인 Deep Learning만 돌리는 경우\n",
    "        - CPU, GPU외에 장비를 사용하는 경우\n",
    "        - 디버깅이 어려움(문서도 아직 갈 길이 멀고, 자잘한 버그도 많음.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d847638-0c5b-43fd-a85f-9fc6ef173349",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic Neural Network\n",
    "using Pkg\n",
    "\n",
    "Pkg.add(\"Flux\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c896ba8-6a4d-4056-88ba-7219761af92e",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: Package Flux not found in current path:\n- Run `import Pkg; Pkg.add(\"Flux\")` to install the Flux package.\n",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package Flux not found in current path:\n- Run `import Pkg; Pkg.add(\"Flux\")` to install the Flux package.\n",
      "",
      "Stacktrace:",
      " [1] require(into::Module, mod::Symbol)",
      "   @ Base .\\loading.jl:967",
      " [2] eval",
      "   @ .\\boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "## Basic NN\n",
    "using Flux\n",
    "\n",
    "NN1 = Chain(Dense(10, 5, tanh),\n",
    "    Dense(5,5, tanh),\n",
    "    Dense(5,2))\n",
    "\n",
    "NN1(rand(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ef8a0-b3d8-4dbf-9f4d-a60cb9d9ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom Activation Function\n",
    "\n",
    "NN2 = Chain(Dense(10, 5, x -> x^2),\n",
    "    Dense(5,5, relu),\n",
    "    Dense(5, 2))\n",
    "\n",
    "NN2(rand(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41fd69-18a2-40cf-8b2f-14fc337e3524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### pytorch에서 이를 구현하려면 좀 더 복잡\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "model1 = nn.Sequential(\n",
    "    nn.Linear(10, 5),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(5,5),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(5,2)v\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae544ede-c0f4-4d5a-88ec-5431b251cefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Neural Network\n",
    "- 모델 training을 위한 기본 요소\n",
    "    - Objective function\n",
    "    - parameters of the model.\n",
    "    - data\n",
    "    - optimaizaer\n",
    "- train!(loss, params, data, opt; cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8208a645-437a-4e5c-8ae1-aa0739f529c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Loss (MSE)\n",
    "    - loss(x, y) = Flux.Losses.mse(NN1(x), y)\n",
    "- Parameters\n",
    "    - ps = Flux.params(NN1)\n",
    "- Optimisers\n",
    "    - opt = Flux.Descent()\n",
    "- Data\n",
    "    - Julia는 Column-major\n",
    "    - Shuffling, mini-batch iteration 기능을 지니는 DataLoader 사용 가능\n",
    "\n",
    "- Training\n",
    "    for epoch_idx in 1:epoch_size\n",
    "        Flux.train!(loss, ps, trian_data, opt)\n",
    "        println(sum(loss.(test_x, test_y)))\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceab6a5-fe68-4a41-874d-2fb22593a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train wiht GPU\n",
    "- Array를 GPU로 변환\n",
    "- Flux model도 GPU로 변환 가능\n",
    "    - RNN의 경우 순차적으로 돌아야 되기 때문에 for 구문을 써야함.\n",
    "    - 각 배치에 대해 broadcasting을 활용해서 쉽게 적용 가능."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
